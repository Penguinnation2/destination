{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd7126d",
   "metadata": {},
   "source": [
    "## Installation \n",
    "pip install -U pip setuptools wheel\n",
    "\n",
    "\n",
    "pip install -U spacy\n",
    "\n",
    "\n",
    "\n",
    "!pip install gensim\n",
    "\n",
    "!pip install pyLDAvis\n",
    "\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559dbb0",
   "metadata": {},
   "source": [
    "## Session 2 \n",
    "1. Tokenization \n",
    "2. Stop words \n",
    "3. Is it a stopword, punc,lpunct,rpunct,etc.\n",
    "4. POS\n",
    "5. DF using token, pos, explain_pos, tag, explain_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f7814",
   "metadata": {},
   "source": [
    "## Session 3 - Spacy Pipeline\n",
    "1. Tokenizer \n",
    "2. Stream of string as inputs - List of string and Tuple of Strings \n",
    "3. Dataframe of sentences \n",
    "4. Splitting the text into sentences \n",
    "5. Tagger \n",
    "6. Pos \n",
    "7. Count of Pos \n",
    "8. Visualization of pos \n",
    "9. Converting a text into a DF with tokens, pos \n",
    "10. Ner and visualization \n",
    "11. Ner with web data \n",
    "12. List of entities \n",
    "13. Entites most apperared "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee69ce",
   "metadata": {},
   "source": [
    "## Session 4 - Paser and Ner\n",
    "1. Tokenizer \n",
    "2. Tagger \n",
    "3. Parser - Visualization \n",
    "4. Noun chunks - Identifies the nouns in sentences \n",
    "5. NER - List of ner\n",
    "6. List of tuples of text and the respective entities\n",
    "7. NER for web data \n",
    "8. Converting string to docs using nlp *Count of entities* \n",
    "9. Entites most apperared \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df7f83",
   "metadata": {},
   "source": [
    "## Session 5 - Rule based matching \n",
    "1. No of sentences in doc\n",
    "2. Tokenization \n",
    "3. Tagger \n",
    "4. Ner \n",
    "5. Rule based matching \n",
    "6. Matching of word - text/lemma\n",
    "7. Occurances of alphabets, digits \n",
    "8. Occurance of words with conditions \n",
    "9. Occurance of ENT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281a496",
   "metadata": {},
   "source": [
    "## Session 6 -  Vectorisation of tokens and similarity of documents\n",
    "1. Creating a list of 4 docs \n",
    "2. Corpus \n",
    "3. Bag of words\n",
    "4. TFIDF Vectorisation\n",
    "5. Similarity of Documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288164ad",
   "metadata": {},
   "source": [
    "## Session 7 - Topic Modeling \n",
    "1. List of multiple docs \n",
    "2. Creating a word list \n",
    "3. Corpus \n",
    "4. Displaying top topics with prob \n",
    "5. Getting topics for a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd52bf36",
   "metadata": {},
   "source": [
    "## Session 8 - Sentiment Analysis \n",
    "1. Identifying sentiments of texts\n",
    "2. Building a model \n",
    "3.from sklearn.model_selection import train_test_split\n",
    "X_train_1,X_test_1,y_train_1,y_test_1=train_test_split(\n",
    "     X_1,y, test_size=0.2,random_state=10)\n",
    "\n",
    "X_train_1.shape,X_test_1.shape,y_train_1.shape,y_test_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a4823",
   "metadata": {},
   "source": [
    "## Session 9 - Text Clustering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee2fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
