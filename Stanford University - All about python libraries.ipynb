{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd7126d",
   "metadata": {},
   "source": [
    "## Installing Spacy \n",
    "pip install -U pip setuptools wheel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pip install -U spacy\n",
    "\n",
    "\n",
    "\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e398e2df",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# !pip install gensim\n",
    "import gensim\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2dense\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "#!pip install textblob\n",
    "from textblob import TextBlob\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559dbb0",
   "metadata": {},
   "source": [
    "##  2 \n",
    "1. Tokenization \n",
    "2. Stop words \n",
    "3. Is it a stopword, punc,lpunct,rpunct,etc.\n",
    "4. POS\n",
    "5. DF using token, pos, explain_pos, tag, explain_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f7814",
   "metadata": {},
   "source": [
    "##  3 - Spacy Pipeline\n",
    "1. Tokenizer \n",
    "2. Stream of string as inputs - List of string and Tuple of Strings \n",
    "3. Dataframe of sentences \n",
    "4. Splitting the text into sentences \n",
    "5. Tagger \n",
    "6. Pos \n",
    "7. Count of Pos \n",
    "8. Visualization of pos \n",
    "9. Converting a text into a DF with tokens, pos \n",
    "10. Ner and visualization \n",
    "11. Ner with web data \n",
    "12. List of entities \n",
    "13. Entites most apperared "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee69ce",
   "metadata": {},
   "source": [
    "##  4 - Paser and Ner\n",
    "1. Tokenizer \n",
    "2. Tagger \n",
    "3. Parser - Visualization \n",
    "4. Noun chunks - Identifies the nouns in sentences \n",
    "5. NER - List of ner\n",
    "6. List of tuples of text and the respective entities\n",
    "7. NER for web data \n",
    "8. Converting string to docs using nlp *Count of entities* \n",
    "9. Entites most apperared \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df7f83",
   "metadata": {},
   "source": [
    "##  5 - Rule based matching \n",
    "1. No of sentences in doc\n",
    "2. Tokenization \n",
    "3. Tagger \n",
    "4. Ner \n",
    "5. Rule based matching \n",
    "6. Matching of word - text/lemma\n",
    "7. Occurances of alphabets, digits \n",
    "8. Occurance of words with conditions \n",
    "9. Occurance of ENT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baed930",
   "metadata": {},
   "source": [
    "##  6 -  Vectorisation of tokens and similarity of documents\n",
    "1. Creating a list of 4 docs \n",
    "2. Corpus \n",
    "3. Bag of words\n",
    "4. TFIDF Vectorisation\n",
    "5. Similarity of Documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c050d",
   "metadata": {},
   "source": [
    "\n",
    "##  7 - Topic Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c9a25",
   "metadata": {},
   "source": [
    "1. List of multiple docs\n",
    "2. Creating a word list\n",
    "3. Corpus\n",
    "4. Displaying top topics with prob\n",
    "5. Getting topics for a word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db5fac",
   "metadata": {},
   "source": [
    "##  8 - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3887b9",
   "metadata": {},
   "source": [
    "1. Identifying sentiments of texts\n",
    "2. Building a model\n",
    "3.from sklearn.model_selection import train_test_split X_train_1,X_test_1,y_train_1,y_test_1=train_test_split( X_1,y, test_size=0.2,random_state=10)\n",
    "\n",
    "X_train_1.shape,X_test_1.shape,y_train_1.shape,y_test_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ed70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
